% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{SSA implementation}\label{chapter:ssa-impl}

The time-series generation tool in addition the the Hidden Markov Model functionality also implements the method of generating time series using Singular Spectrum Analysis discussed above. We will look at the Python implementations of the different steps of the SSA algorithm. \parencite{tsgenerator}

\section{Generation}

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def handle_ssa(orig, window, components, l):
    X = parseCSV(orig).reshape(-1)
    n = X.size

    C = embed(X, window)
    Xcomp, eigenv = decompose(C)

    x_tilde = reconstruct(Xcomp, components)

    R = compute_R(eigenv, components)

    additional_samples = max(0, l-n)
    fc = forecast(x_tilde, R, components, additional_samples)
    return fc[:l]
\end{lstlisting}
\end{singlespace}
\caption{SSA handle\_ssa function}    
\label{fig:ssa-handle}
\end{figure}

The high level function handling the time-series generation with SSA is called "handle\_ssa" and is shown in Figure \ref{fig:ssa-handle}. The parameters are "orig", the original time-series that is going to be forecasted, "windows", the window length $L$, "components", the number of components $K$, and "l", the requested length of the time-series about to be generated. 

Since this is a high level function that gets called directly with the commandline arguments passed by the user, orig is actually a path to a file containing the original time-series. We obtain a numpy array of the time-series from "parseCSV". 

The function then follows the steps outlined in the theory section above. First we compute the trajectory matrix, named "C" here, using the embed function. Then we decompose the trajectory matrix to obtain the different component matrices and the corresponding eigenvectors. 

Next we obtain the reconstructed series "x\_tilde" from the components we get from the decomposition. Then the recurrence vector "R" is computed from the eigenvectors. 

The variable "additional\_samples" holds the number of steps we have to forecast to reach the requested time-series length for the generation. Since it is possible to request a shorter series than the original, this is floored at 0. 

Finally the forecast function returns the concatenation of the approximation of the original time series and the forecasted samples. When returning this as the result of the function if it is longer than the desired length "l". 

\section{Decomposition}

Figure \ref{fig:ssa-embed} show the function "embed". It computes the trajectory matrix from time-series X using a window length of k. The matrix will have size $k \times l-k+1$. "l" being the length of X. 

Figure \ref{fig:ssa-decompose} shows the decompose function, which decomposes the trajectory matrix into component matrices one per eigenvector.  We first obtain the rank of the matrix since the rank determines the number of eigenvectors a matrix has. Then we apply Singular Value Decomposition on the square of X. Only the first two result of the SVD are useful to us: the matrix U, whose columns are the eigenvectors and S, which contains the eigenvalues. 

We than loop through all the eigenvectors. In each iteration we extract the eigenvector from the U matrix and compute P and Q according to their definitions. The component matrix X is the outer product of the vectors P and Q. 

Since we need the eigenvectors later for the computation of R, they get collected in an array and returned together with an array of component matrices. 

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def embed(X, k):
    l = X.size
    C = np.zeros((k, l - k + 1))
    for i in range(k):
        for j in range(l - k + 1):
            C[i, j] = X[j+i]
    return C
\end{lstlisting}
\end{singlespace}
\caption{SSA embed function}    
\label{fig:ssa-embed}
\end{figure}

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def decompose(X):
    rank = np.linalg.matrix_rank(X)
    Xsq = X @ X.T
    U, S, _ = np.linalg.svd(Xsq)
    s = np.sqrt(S)

    Xcomp = []
    eigenvectors = []

    for i in range(rank):
        egv = U[:, i]
        lamb = s[i]
        Q = X.T@(egv/lamb)
        P = lamb*egv
        Xcomp += [np.outer(P, (Q.T))]
        eigenvectors += [egv]
    return Xcomp, eigenvectors
\end{lstlisting}
\end{singlespace}
\caption{SSA decompose function}    
\label{fig:ssa-decompose}
\end{figure}

\section{Reconstruction}

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def diagonal_avg(M):
    k, l = M.shape
    res = []
    for i in range(l + k - 1):
        count = 0
        s = 0
        for j in range(k):
            x = j
            y = i - j
            if x >= 0 and x < k and y >= 0 and y < l:
                count += 1
                s += M[x, y]
        res += [s/count]
    return res
\end{lstlisting}
\end{singlespace}
\caption{SSA diagonal averaging function}    
\label{fig:ssa-diag-avg}
\end{figure}

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def reconstruct(Xcomp, ncomponents):
    Xfull = np.zeros(Xcomp[0].shape)
    for i in range(ncomponents):
        X = Xcomp[i]
        Xfull += X

    x_tilde = diagonal_avg(Xfull)
    return x_tilde
\end{lstlisting}
\end{singlespace}
\caption{SSA reconstruct function}    
\label{fig:ssa-reconstruct}
\end{figure}

Next up the the reconstruction step. Figure \ref{fig:ssa-reconstruct} shows the function implementing this. The parameters are Xcomp, the component matrices obtained above and ncomponents, the specified number of components to use. Then we add up the first ncomponents of the components to obtain the reconstructed matrix Xfull. To go from matrix back to time-series the diagonal\_avg function is applied. 

Note that it is not necessary to do these operations in this order of first summing up the matrices then converting to a time-series. The other way of first converting all the components to time-series and then summing up those is equally correct. 

Figure \ref{fig:ssa-diag-avg} shows the implementation of the diagonal averaging function. It is the inverse of the "embed" function. It takes a matrix M of size $k \times l$ and turns it into a array of length $l+k-1$. This works by averaging the values of the anti-diagonals. 

\section{Forecasting}

\begin{figure}
\begin{singlespace}
\begin{lstlisting}[language=Python]
def compute_R(eigenv, ncomponents):
    vsq = 0
    R = np.zeros(eigenv[0].size-1)
    for i in range(ncomponents):
        eig = eigenv[i]
        pi = eig[-1]
        vsq += pi**2
        R += pi * eig[:-1]
    R = R / (1 - vsq)
    return R

def forecast(x_tilde, R, steps):
    n = len(x_tilde)
    d = R.size
    new_series = x_tilde[:]
    for i in range(steps):
        x = R.T @ new_series[i+n-d:i+n]
        new_series += [x]
    return new_series
\end{lstlisting}
\end{singlespace}
\caption{SSA forecast function}    
\label{fig:ssa-forecast}
\end{figure}

The last step to implement is the forecasting. The two functions related to forecasting "compute\_R" and "forecast" are listed in Figure \ref{fig:ssa-forecast}. 

To forecast we need the recurrence vector R, which is calculated by the function "compute\_R". The parameters are the eigenvectors and the chosen number of components ncomponents. The R vector's length is one less than the length of the eigenvectors. 

The definition of R in Equation \eqref{eq:r-def} contains two sums over the chosen grouping I. In our case that is always the eigenvectors 1 through ncomponents. The two sums get computed first and stored in "vsq" and "R". The terms  $\underline{P_i}$ and $\pi$ from the definition are expressed in code as "eig[:-1]" and "eig[-1]" respectively. 

Having computed R we can then do the forecast using function "forecast". The arguments for this function are "x\_tilde", the reconstructed series, "R", the recurrence vector, and "steps", the amount of steps to forecast. 

"forecast" will return one array containing both the reconstructed time-series and the forecasted steps concatenated together. For this we first create a shallow copy of "x\_tilde" in "new\_series". Then according to the number of additional steps required we append forecasted values. Each new value is the dot product of $R^T$ and the current last d entries in "new\_series". "d" being the length of the R vector. 

With this we have covered all the relevant code for time-series generation using SSA in the time series generation tool. 